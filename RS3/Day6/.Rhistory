library(readr)
dataset <- read_csv(NULL)
library(readr)
USStamps <- read_csv("USStamps.csv")
View(USStamps)
library(readr)
Pines <- read_csv("Pines.csv")
View(Pines)
library(readr)
BreesPass <- read_csv("BreesPass.csv")
View(BreesPass)
library(Stat2Data)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(effectsize)
library(readr)
library(Stat2Data)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(effectsize)
library(readr)
# 2.9:
#a. A high r^2 can occur when the data have a strong relationship but follow a curved trend instead of a straight line. In such cases, a linear model explains much of the variability but doesn’t fully capture the pattern. Transforming the variables to straighten the curve could produce a more accurate fit.
# b. A low r^2 doesn’t necessarily imply the linear model is unsuitable. If the true relationship is a straight line but the data points scatter widely due to random error, the r^2 will appear low. This reflects noise in the data, not a flaw in using a linear model.
# 2.23:
USstamps <- read_csv("USStamps.csv")
stamps <- USstamps[!(USstamps$Year %in% c(1885, 1917, 1919, 1932)), ]
stamp_mod <- lm(Price ~ Year, data = stamps)
summary(stamp_mod)
# a. With R^2 ≈98.5%, nearly all variation in stamp prices is explained by year, showing that time is an excellent predictor of the trend.
# b. The slope’s very high t-value and near-zero p-value provide strong evidence of a robust linear link between postage costs and year.
# c. The ANOVA results, featuring a massive F-statistic and negligible p-value, confirm that the regression on year explains almost all stamp price variation, with little unexplained error.
# 2.27
BreesPass <- read_csv("BreesPass.csv")
brees_mod <- lm(Yards ~ Attempts, data = BreesPass)
summary(brees_mod)
plot(BreesPass$Attempts, BreesPass$Yards, pch=19, col="blue")
abline(brees_mod, col="red", lwd=2)
# a. The regression line is Yards = 86.1 + 5.69 × Attempts, showing a moderately positive trend where more pass attempts generally correspond to more yards.
# b. The season average of about 7.7 yards per attempt exceeds the slope of 5.69, since the regression slope reflects the game-level relationship rather than the overall per-pass ratio.
# c. An R^2 of roughly 0.34 indicates that about 34% of the variation in Brees’s passing yards can be attributed to attempts, with most variation explained by other influences.
# 2.31
Pines <- read_csv("Pines.csv")
pine_mod <- lm(Hgt97 ~ Hgt90, data = Pines)
summary(pine_mod)
anova(pine_mod)
# a. The regression slope is highly significant with an extremely small p-value, indicating a clear link between 1990 starting height and 1997 tree height.
# b. With an R^2 of only about 2.7%, the 1990 height explains very little of the later variation, so the model has weak predictive value.
# c. The ANOVA results split the total variation into regression SS (≈138,344) and residual SS (≈5,010,010), underscoring the model’s limited explanatory power.
# d. Calculating regression SS over total SS yields about 0.027, consistent with the reported R^2, confirming agreement between the ANOVA and summary output.
# e. Although statistically significant, the model explains so little variation that it’s not practically useful for predicting 1997 heights.
library(readr)
TextPrices <- read_csv("TextPrices.csv")
View(TextPrices)
library(tidyverse)
library(readr)
# 2.44
TextPrices <- read_csv("TextPrices.csv")
model_textbooks <- lm(Price ~ Pages, data = TextPrices)
predict(model_textbooks, newdata = data.frame(Pages = 450),
interval = "confidence", level = 0.95)
# a. The 95% confidence interval for the mean cost of a 450-page textbook ranges from about $51.73 to $74.02, indicating strong confidence that the true average price of such books lies within this interval.
predict(model_textbooks, newdata = data.frame(Pages = 450),
interval = "prediction", level = 0.95)
# b. For an individual 450-page textbook, the 95% prediction interval spans about $0.90 to $124.85, reflecting the larger spread in single prices versus the mean.
# c. Both intervals share the same midpoint (~$62.88), as they center on the regression model’s predicted value.
# d. The prediction interval is noticeably wider than the confidence interval because it incorporates individual price variation, not just uncertainty in the average.
# e. The narrowest prediction interval occurs near the dataset’s mean page count (~464.5), where variation from distance to the average is minimized.
# f. The 95% prediction interval for a 1500-page textbook is about $143.36 to $291.78. But since 1500 pages lies far outside the data’s observed range, this is extrapolation, and the model’s reliability here is questionable.
# 2.47
library(tidyverse)
library(readr)
# 2.44
TextPrices <- read_csv("TextPrices.csv")
model_textbooks <- lm(Price ~ Pages, data = TextPrices)
predict(model_textbooks, newdata = data.frame(Pages = 450),
interval = "confidence", level = 0.95)
# a. The 95% confidence interval for the mean cost of a 450-page textbook ranges from about $51.73 to $74.02, indicating strong confidence that the true average price of such books lies within this interval.
predict(model_textbooks, newdata = data.frame(Pages = 450),
interval = "prediction", level = 0.95)
# b. For an individual 450-page textbook, the 95% prediction interval spans about $0.90 to $124.85, reflecting the larger spread in single prices versus the mean.
# c. Both intervals share the same midpoint (~$62.88), as they center on the regression model’s predicted value.
# d. The prediction interval is noticeably wider than the confidence interval because it incorporates individual price variation, not just uncertainty in the average.
# e. The narrowest prediction interval occurs near the dataset’s mean page count (~464.5), where variation from distance to the average is minimized.
# f. The 95% prediction interval for a 1500-page textbook is about $143.36 to $291.78. But since 1500 pages lies far outside the data’s observed range, this is extrapolation, and the model’s reliability here is questionable.
# 2.47
RailsTrails <- read_csv("RailsTrails.csv")
model_rails <- lm(adj2007 ~ distance, data = RailsTrails)
predict(model_rails, newdata = data.frame(distance = 0.5))
# a. A house half a mile from the trail is predicted to sell for ~$360,990.
predict(model_rails, newdata = data.frame(distance = 0.5),
interval = "prediction", level = 0.90)
# b. The 90% prediction interval ranges from about $207,017 to $514,964, indicating that most homes half a mile away are expected to fall within this price bracket.
plot(model_rails, which = 1)
# c. The residual plot shows unequal variance, suggesting the constant variance assumption is violated, which reduces the reliability of the intervals in (a) and (b).
model_log <- lm(log(adj2007) ~ log(distance), data = RailsTrails)
predict(model_log, newdata = data.frame(distance = 0.5),
interval = "prediction", level = 0.90)
# d. With a log transformation, the 90% prediction interval is (5.36, 6.21) in log-dollars, or roughly $211,866 to $499,049 after conversion. This interval is narrower, meaning the logged model provides more precise predictions.
plot(model_log, which = 1)
# e. The log-transformed model shows much more stable variance, so assumptions are met better. The interval back on the dollar scale is narrower, reflecting the improved model fit.
library(readr)
BaseballTimes2017 <- read_csv("BaseballTimes2017.csv")
View(BaseballTimes2017)
library(readr)
RailsTrails <- read_csv("~/Downloads/School/12th_Grade/RS3/Day7/RailsTrails.csv")
View(RailsTrails)
# 3.1
# a. A student with a midterm score of 100 and a project score of 30 would have a predicted final exam score of: Final = 11.0 + 0.53(100) + 1.20(30) = 100. In short, these inputs yield an expected final of 100.
# b. For Michael, with a midterm of 87 and a project score of 21, the predicted final is about 82.31. Since he actually earned 80, his residual is -2.31, meaning he scored about 2.3 points below expectation.
# 3.3
# The project coefficient is larger than the midterm’s, but that alone doesn’t prove greater importance. To assess impact, we must also consider variability and standard errors. It may be that the midterm has a stronger overall relationship with the final exam even though its coefficient is smaller.
# 3.5
# If the midterm score is fixed, each additional project point adds about 1.2 points to the predicted final exam score.
# 3.7
# a. True. Since total variance uses more degrees of freedom (n−1) than error variance (n−k−1), the R-squared formula is well-defined and interpretable.
# b. False. Adding a weak predictor can slightly increase error per degree of freedom. The penalty for including it may outweigh any small gain, lowering adjusted R-squared.
# 3.17
# a. The t-value for weight is about 1.08, with a p-value of 0.282. This high p-value suggests no strong evidence that weight affects active pulse once resting pulse and exercise are accounted for.
# b. With 232 cases and 4 parameters, the df is 228. Using t ≈ 1.65 for 90% confidence, the weight coefficient CI is roughly (-0.018, 0.087). Interpretation: a 1-pound increase could slightly decrease or slightly increase active pulse, but zero is in the range, so weight may have no real effect.
# c. For Resting = 76, Weight = 200, and Exercise = 7, the predicted active pulse is about 96 bpm.
library(tidyverse)
library(readr)
# 3.18
RailsTrails <- read_csv("RailsTrails.csv")
model_simple <- lm(adj2007 ~ distance, data = RailsTrails)
summary(model_simple)
# a. In the simple regression, every additional mile from the trail is linked to about a $54,127 drop in home price. This effect is statistically significant, with the model explaining roughly 24% of price variation. The average prediction error is around $92,130.
model_multiple <- lm(adj2007 ~ distance + squarefeet, data = RailsTrails)
summary(model_multiple)
# b. After adding square footage, the distance effect weakens to about $16,486 per mile but remains significant. Accounting for home size greatly improves the model: R² increases from 0.237 to 0.766, and the standard error falls to $51,340.
moe_simple <- qt(0.975, df = 102) * 9.659
ci_simple <- c(-54.4272 - moe_simple, -54.4272 + moe_simple)
ci_simple
moe_multiple <- qt(0.975, df = 101) * 5.942
ci_multiple <- c(-16.486 - moe_multiple, -16.486 + moe_multiple)
ci_multiple
# c. The simple model’s 95% CI for the distance effect spans about -73.59 to -35.27 (thousands). With square footage included, it tightens to roughly -28.27 to -4.61, showing that size reduces both the estimated impact and the uncertainty.
# d. For a 1,500-square-foot house located half a mile from the trail, the model predicts a selling price of about $226,217, reflecting the combined influence of size and proximity.
